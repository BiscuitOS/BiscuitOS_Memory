From 19d84d6e92ce622a87c819ed1bf23f23a2a647bf Mon Sep 17 00:00:00 2001
From: BuddyZhang1 <buddy.zhang@aliyun.com>
Date: Tue, 5 May 2020 16:02:01 +0800
Subject: [PATCH] add percpu alloc and free

---
 RunBiscuitOS_mm.sh         |  2 +-
 include/biscuitos/percpu.h | 46 ++++++++++++++++++++++++++
 mm/slab.c                  | 67 ++++++++++++++++++++++++++++++++++++++
 modules/percpu/main.c      | 34 +++++++++++++++++++
 4 files changed, 148 insertions(+), 1 deletion(-)

diff --git a/RunBiscuitOS_mm.sh b/RunBiscuitOS_mm.sh
index e1f22c8..c882ae6 100755
--- a/RunBiscuitOS_mm.sh
+++ b/RunBiscuitOS_mm.sh
@@ -53,7 +53,7 @@ show_trace()
 
 mount_fs()
 {
-	insmod /lib/modules/$(uname -r)/extra/mm_bs-0.0.1.ko
+	insmod /lib/modules/$(uname -r)/extra/BiscuitOS_MMU-2.6.12.ko
 }
 
 umount_fs()
diff --git a/include/biscuitos/percpu.h b/include/biscuitos/percpu.h
index 63cbbd9..6342346 100644
--- a/include/biscuitos/percpu.h
+++ b/include/biscuitos/percpu.h
@@ -1,6 +1,8 @@
 #ifndef _BISCUITOS_PERCPU_H
 #define _BISCUITOS_PERCPU_H
 
+#include "asm-generated/arch.h"
+
 /* Enough to cover all DEFINE_PER_CPUs in kernel, including modules. */
 #ifndef PERCPU_ENOUGH_ROOM_BS
 #define PERCPU_ENOUGH_ROOM_BS	32768
@@ -10,4 +12,48 @@
 #define get_cpu_var_bs(var) (*({ preempt_disable(); &__get_cpu_var_bs(var); }))
 #define put_cpu_var_bs(var) preempt_enable()
 
+#ifdef CONFIG_SMP
+
+struct percpu_data_bs {
+	void *ptrs[NR_CPUS_BS];
+	void *blkp;
+};
+
+/* 
+ * Use this to get to a cpu's version of the per-cpu object allocated using
+ * alloc_percpu.  Non-atomic access to the current CPU's version should
+ * probably be combined with get_cpu()/put_cpu().
+ */
+#define per_cpu_ptr_bs(ptr, cpu)					\
+({									\
+	struct percpu_data_bs *__p = 					\
+			(struct percpu_data_bs *)~(unsigned long)(ptr);	\
+	(__typeof__(ptr))__p->ptrs[(cpu)];				\
+})
+
+extern void *__alloc_percpu_bs(size_t size, size_t align);
+extern void free_percpu_bs(const void *);
+
+#else /* CONFIG_SMP */
+
+#define per_cpu_ptr_bs(ptr, cpu) (ptr)
+
+static inline void *__alloc_percpu_bs(size_t size, size_t align)
+{
+	void *ret = kmalloc_bs(size, GFP_KERNEL_BS);
+	if (ret)
+		memset(ret, 0, size);
+	return ret;
+}
+static inline void free_percpu_bs(const void *ptr)
+{
+	kfree_bs(ptr);
+}
+
+#endif /* CONFIG_SMP */
+
+/* Simple wrapper for the common case: zeros memory. */
+#define alloc_percpu_bs(type) \
+	((type *)(__alloc_percpu_bs(sizeof(type), __alignof__(type))))
+
 #endif
diff --git a/mm/slab.c b/mm/slab.c
index 553a3c6..89cd2ba 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2691,6 +2691,73 @@ void __init kmem_cache_init_bs(void)
 	DEBUG_CALL(slab);
 }
 
+#ifdef CONFIG_SMP
+/**
+ * __alloc_percpu - allocate one copy of the object for every present
+ * cpu in the system, zeroing them.
+ * Objects should be dereferenced using the per_cpu_ptr macro only.
+ *
+ * @size: how many bytes of memory are required.
+ * @align: the alignment, which can't be greater than SMP_CACHE_BYTES.
+ */
+void *__alloc_percpu_bs(size_t size, size_t align)
+{
+	int i;
+	struct percpu_data_bs *pdata = 
+			kmalloc_bs(sizeof (*pdata), GFP_KERNEL_BS);
+
+	if (!pdata)
+		return NULL;
+
+	for (i = 0; i < NR_CPUS_BS; i++) {
+		if (!cpu_possible(i))
+			continue;
+		pdata->ptrs[i] = kmalloc_node_bs(size, GFP_KERNEL_BS,
+						cpu_to_node_bs(i));
+
+		if (!pdata->ptrs[i])
+			goto unwind_oom;
+		memset(pdata->ptrs[i], 0, size);
+	}
+
+	/* Catch derefs w/o wrappers */
+	return (void *) (~(unsigned long) pdata);
+
+unwind_oom:
+	while (--i >= 0) {
+		if (!cpu_possible(i))
+			continue;
+		kfree_bs(pdata->ptrs[i]);
+	}
+	kfree_bs(pdata);
+	return NULL;
+}
+EXPORT_SYMBOL(__alloc_percpu_bs);
+
+/**
+ * free_percpu - free previously allocated percpu memory
+ * @objp: pointer returned by alloc_percpu.
+ *
+ * Don't free memory not originally allocated by alloc_percpu()
+ * The complemented objp is to check for that.
+ */
+void
+free_percpu_bs(const void *objp)
+{
+	int i;
+	struct percpu_data_bs *p = 
+			(struct percpu_data_bs *) (~(unsigned long) objp);
+
+	for (i = 0; i < NR_CPUS_BS; i++) {
+		if (!cpu_possible(i))
+			continue;
+		kfree_bs(p->ptrs[i]);
+	}
+	kfree_bs(p);
+}
+EXPORT_SYMBOL_GPL(free_percpu_bs);
+#endif
+
 static int __init __unused cpucache_init_bs(void)
 {
 	int cpu;
diff --git a/modules/percpu/main.c b/modules/percpu/main.c
index 0fba5d1..c84c156 100644
--- a/modules/percpu/main.c
+++ b/modules/percpu/main.c
@@ -10,6 +10,7 @@
 #include <linux/kernel.h>
 #include <linux/errno.h>
 #include <linux/smp.h>
+#include <linux/cpumask.h>
 #include "biscuitos/kernel.h"
 #include "biscuitos/init.h"
 #include "biscuitos/percpu.h"
@@ -55,3 +56,36 @@ static int __unused TestCase_percpu(void)
 	return 0;
 }
 percpu_initcall_bs(TestCase_percpu);
+
+/*
+ * TestCase: alloc_percpu/free_percpu
+ */
+static int __unused TestCase_alloc_percpu(void)
+{
+	struct node_percpu __percpu *np, *ptr;
+	int cpu;
+
+	/* Allocate percpu */
+	np = alloc_percpu_bs(struct node_percpu);
+	if (!np) {
+		printk("%s __alloc_percpu failed.\n", __func__);
+		return -ENOMEM;
+	}
+
+	/* setup */
+	for_each_possible_cpu(cpu) {
+		ptr = per_cpu_ptr_bs(np, cpu);
+		ptr->index = cpu * 0x10;
+	}
+
+	/* usage */
+	for_each_possible_cpu(cpu) {
+		ptr = per_cpu_ptr_bs(np, cpu);
+		bs_debug("CPU-%d Index %#lx\n", cpu, ptr->index);
+	}
+	
+	/* free percpu */
+	free_percpu_bs(np);
+	return 0;
+}
+slab_initcall_bs(TestCase_alloc_percpu);
-- 
2.17.1

